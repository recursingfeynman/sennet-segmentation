{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f668458",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-06T13:38:27.220436Z",
     "iopub.status.busy": "2024-02-06T13:38:27.220110Z",
     "iopub.status.idle": "2024-02-06T13:39:28.206041Z",
     "shell.execute_reply": "2024-02-06T13:39:28.205192Z"
    },
    "papermill": {
     "duration": 60.994626,
     "end_time": "2024-02-06T13:39:28.208487",
     "exception": false,
     "start_time": "2024-02-06T13:38:27.213861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.8.0 requires botocore<1.33.2,>=1.32.4, but you have botocore 1.34.35 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "try:\n",
    "    import angionet\n",
    "except ImportError:\n",
    "    GITHUB_TOKEN = secrets.get_secret(\"github-token\")\n",
    "    USERNAME = secrets.get_secret(\"github-username\")\n",
    "    URL = f\"https://{USERNAME}:{GITHUB_TOKEN}@github.com/{USERNAME}/sennet-segmentation.git\"\n",
    "\n",
    "    os.system(f\"pip install -q git+{URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83acd4ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:39:28.220343Z",
     "iopub.status.busy": "2024-02-06T13:39:28.220060Z",
     "iopub.status.idle": "2024-02-06T13:39:42.494648Z",
     "shell.execute_reply": "2024-02-06T13:39:42.493846Z"
    },
    "papermill": {
     "duration": 14.283259,
     "end_time": "2024-02-06T13:39:42.497132",
     "exception": false,
     "start_time": "2024-02-06T13:39:28.213873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch as AP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from angionet.core import evaluate, train, predict\n",
    "from angionet.datasets import TrainDataset, InferenceDataset\n",
    "from angionet.metrics import dice, summary\n",
    "from angionet.utils import set_seed, visualize\n",
    "from angionet.functional import standardize, rescale, decode, colorize\n",
    "from angionet.postprocessing import fill_holes, apply_threshold\n",
    "\n",
    "\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "\n",
    "class Rescale(ImageOnlyTransform):\n",
    "    def __init__(self, always_apply=True, p = 1):\n",
    "        super().__init__(self)\n",
    "\n",
    "    def apply(self, image, **kwargs):\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        return np.asarray(image, dtype = 'float32')\n",
    "    \n",
    "class NormalizeClip(ImageOnlyTransform):\n",
    "    def __init__(self, always_apply=True, p = 1):\n",
    "        super().__init__(self)\n",
    "        \n",
    "    def apply(self, image, **kwargs):\n",
    "        image = (image - image.mean()) / (image.std() + 1e-8)\n",
    "        image = np.clip(image, a_min = -3, a_max = 5)\n",
    "        return np.asarray(image, dtype = 'float32')\n",
    "    \n",
    "class Noise(ImageOnlyTransform):\n",
    "    def __init__(self, always_apply=True, p = 1, normalize = False, max_random_rate = 0.1):\n",
    "        super().__init__(self)\n",
    "        self.normalize = normalize\n",
    "        self.max_random_rate = max_random_rate\n",
    "        \n",
    "    def apply(self, image, **kwargs):\n",
    "        if self.normalize:\n",
    "            xstd = image.std()\n",
    "            xmean = image.mean()\n",
    "        else:\n",
    "            xstd = np.ones((1, 1))\n",
    "            xmean = np.zeros((1, 1))\n",
    "        random_rate = self.max_random_rate * np.random.rand() * np.random.rand(*xmean.shape)\n",
    "        cache = np.sqrt(xstd ** 2 + (xstd * random_rate) ** 2)\n",
    "        image = (image - xmean + np.random.randn(*image.shape) * random_rate * xstd) / (cache + 1e-7)\n",
    "        return np.asarray(image, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec088fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:39:42.509145Z",
     "iopub.status.busy": "2024-02-06T13:39:42.508634Z",
     "iopub.status.idle": "2024-02-06T13:39:42.525164Z",
     "shell.execute_reply": "2024-02-06T13:39:42.524368Z"
    },
    "papermill": {
     "duration": 0.024793,
     "end_time": "2024-02-06T13:39:42.527320",
     "exception": false,
     "start_time": "2024-02-06T13:39:42.502527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    seed           = 42\n",
    "    root           = \"/kaggle/input/blood-vessel-segmentation\"\n",
    "    data           = [\n",
    "                        \"/kaggle/input/sennet-slicing-hxw\",\n",
    "                        \"/kaggle/input/sennet-slicing-dxh\",\n",
    "                        \"/kaggle/input/sennet-slicing-dxw\",\n",
    "                     ]\n",
    "    batch_size     = 6\n",
    "    epochs         = 25\n",
    "    thresholds     = (0.2, 0.5)\n",
    "    dim            = 'full'\n",
    "    backbone       = \"convnext-tiny\"\n",
    "    train          = ['kidney_1_dense', 'kidney_3_dense']\n",
    "    test           = ['kidney_2']\n",
    "    split          = (6, 1) # stride    \n",
    "    accumulate     = 3\n",
    "    learning_rate  = 5e-4\n",
    "    weight_decay   = 1e-5\n",
    "    clipnorm       = 6\n",
    "\n",
    "    transforms = {\n",
    "        \"train\": A.Compose([\n",
    "            A.CLAHE(p = 1, always_apply = True),\n",
    "            Rescale(),\n",
    "            A.HorizontalFlip(),\n",
    "            A.VerticalFlip(),\n",
    "            A.PadIfNeeded(1312, 1312),\n",
    "            A.CenterCrop(1312, 1312),\n",
    "            A.GridDistortion(),\n",
    "            A.ShiftScaleRotate(p = 0.3),\n",
    "            A.RandomBrightnessContrast(p = 1),\n",
    "            Noise(),\n",
    "            NormalizeClip(),\n",
    "            A.RandomRotate90(p = 1),\n",
    "            A.HorizontalFlip(p = 1),\n",
    "            A.VerticalFlip(p = 1),\n",
    "            AP.ToTensorV2()\n",
    "        ]),\n",
    "        \n",
    "        \"test\": A.Compose([\n",
    "            A.CLAHE(p = 1, always_apply = True),\n",
    "            Rescale(),\n",
    "            A.PadIfNeeded(1728, 1536, \n",
    "                          position = A.PadIfNeeded.PositionType.TOP_LEFT, \n",
    "                          border_mode = cv2.BORDER_CONSTANT),\n",
    "            NormalizeClip(),\n",
    "            AP.ToTensorV2()\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_dict():\n",
    "        return {\n",
    "            key:value \n",
    "            for key, value in vars(config).items() \n",
    "            if not key.startswith('__') and not callable(value)\n",
    "        }\n",
    "    \n",
    "set_seed(seed = config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335da32d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:39:42.538863Z",
     "iopub.status.busy": "2024-02-06T13:39:42.538204Z",
     "iopub.status.idle": "2024-02-06T13:39:47.461419Z",
     "shell.execute_reply": "2024-02-06T13:39:47.460324Z"
    },
    "papermill": {
     "duration": 4.931939,
     "end_time": "2024-02-06T13:39:47.464229",
     "exception": false,
     "start_time": "2024-02-06T13:39:42.532290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vessels</th>\n",
       "      <th>group</th>\n",
       "      <th>image</th>\n",
       "      <th>path</th>\n",
       "      <th>axis</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>kidney_1_dense_0289</td>\n",
       "      <td>151067 1 151978 2 152781 1 152890 2 153692 2 1...</td>\n",
       "      <td>kidney_1_dense</td>\n",
       "      <td>289</td>\n",
       "      <td>/kaggle/input/sennet-slicing-dxw/images/kidney...</td>\n",
       "      <td>DxW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.363207</td>\n",
       "      <td>0.045618</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>kidney_3_dense_0689</td>\n",
       "      <td>934 7 2640 7 4346 7 6053 6 7759 7 9465 7 11171...</td>\n",
       "      <td>kidney_3_dense</td>\n",
       "      <td>689</td>\n",
       "      <td>/kaggle/input/sennet-slicing-dxh/images/kidney...</td>\n",
       "      <td>DxH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.303010</td>\n",
       "      <td>0.013312</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>kidney_1_dense_0745</td>\n",
       "      <td>142560 1 143472 1 144384 1 145296 2 146208 2 1...</td>\n",
       "      <td>kidney_1_dense</td>\n",
       "      <td>745</td>\n",
       "      <td>/kaggle/input/sennet-slicing-dxw/images/kidney...</td>\n",
       "      <td>DxW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.584314</td>\n",
       "      <td>0.367913</td>\n",
       "      <td>0.039761</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>kidney_1_dense_0541</td>\n",
       "      <td>135906 1 137209 2 138512 2 191983 2 193286 3 1...</td>\n",
       "      <td>kidney_1_dense</td>\n",
       "      <td>541</td>\n",
       "      <td>/kaggle/input/sennet-slicing-dxh/images/kidney...</td>\n",
       "      <td>DxH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.375467</td>\n",
       "      <td>0.043344</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>kidney_2_1965</td>\n",
       "      <td>452586 1 454096 3 455608 1 478208 1 478245 2 4...</td>\n",
       "      <td>kidney_2</td>\n",
       "      <td>1965</td>\n",
       "      <td>/kaggle/input/sennet-slicing-hxw/images/kidney...</td>\n",
       "      <td>HxW</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>0.396078</td>\n",
       "      <td>0.756863</td>\n",
       "      <td>0.469863</td>\n",
       "      <td>0.033685</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                            vessels  \\\n",
       "1835  kidney_1_dense_0289  151067 1 151978 2 152781 1 152890 2 153692 2 1...   \n",
       "1490  kidney_3_dense_0689  934 7 2640 7 4346 7 6053 6 7759 7 9465 7 11171...   \n",
       "2063  kidney_1_dense_0745  142560 1 143472 1 144384 1 145296 2 146208 2 1...   \n",
       "1178  kidney_1_dense_0541  135906 1 137209 2 138512 2 191983 2 193286 3 1...   \n",
       "3750        kidney_2_1965  452586 1 454096 3 455608 1 478208 1 478245 2 4...   \n",
       "\n",
       "               group  image  \\\n",
       "1835  kidney_1_dense    289   \n",
       "1490  kidney_3_dense    689   \n",
       "2063  kidney_1_dense    745   \n",
       "1178  kidney_1_dense    541   \n",
       "3750        kidney_2   1965   \n",
       "\n",
       "                                                   path axis  height   width  \\\n",
       "1835  /kaggle/input/sennet-slicing-dxw/images/kidney...  DxW     NaN     NaN   \n",
       "1490  /kaggle/input/sennet-slicing-dxh/images/kidney...  DxH     NaN     NaN   \n",
       "2063  /kaggle/input/sennet-slicing-dxw/images/kidney...  DxW     NaN     NaN   \n",
       "1178  /kaggle/input/sennet-slicing-dxh/images/kidney...  DxH     NaN     NaN   \n",
       "3750  /kaggle/input/sennet-slicing-hxw/images/kidney...  HxW  1041.0  1511.0   \n",
       "\n",
       "           min       max      mean       std  stage  \n",
       "1835  0.133333  0.749020  0.363207  0.045618  train  \n",
       "1490  0.266667  0.486275  0.303010  0.013312  train  \n",
       "2063  0.160784  0.584314  0.367913  0.039761  train  \n",
       "1178  0.156863  0.882353  0.375467  0.043344  train  \n",
       "3750  0.396078  0.756863  0.469863  0.033685   test  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for path in config.data:\n",
    "    data.append(pd.read_csv(Path(path, \"images/train_rles.csv\")))\n",
    "    \n",
    "data = pd.concat(data)[['id', 'vessels', 'group', 'image', \n",
    "                        'path', 'axis', 'height', 'width', \n",
    "                        'min', 'max','mean', 'std']]\n",
    "\n",
    "for groups, stage in zip([config.train, config.test], ['train', 'test']):\n",
    "    data.loc[data.group.isin(groups), 'stage'] = stage\n",
    "\n",
    "dirs = {g:p for g, p in zip([\"HxW\", \"DxH\", \"DxW\"], config.data)}\n",
    "data['path'] = data.apply(lambda x: f\"{dirs[x.axis]}/{x.path}\", axis = 1)\n",
    "data = data.dropna(subset = ['stage'])\n",
    "\n",
    "train_ids = data.query(\"stage == 'train'\")['id'].iloc[::config.split[0]]\n",
    "train_data = data.loc[(data.stage == 'train') & (data.id.isin(train_ids))]\n",
    "test_data = data.loc[(data.stage == 'test') & (data.axis == 'HxW') & (data.image > 900)]\n",
    "data = pd.concat((train_data, test_data), axis = 0).reset_index(drop=True)\n",
    "\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a47e0972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:39:47.477658Z",
     "iopub.status.busy": "2024-02-06T13:39:47.477337Z",
     "iopub.status.idle": "2024-02-06T13:39:47.484293Z",
     "shell.execute_reply": "2024-02-06T13:39:47.483436Z"
    },
    "papermill": {
     "duration": 0.015857,
     "end_time": "2024-02-06T13:39:47.486497",
     "exception": false,
     "start_time": "2024-02-06T13:39:47.470640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HiPDataset(Dataset):\n",
    "    def __init__(self, paths, rles, transforms):\n",
    "        self.paths = paths\n",
    "        self.rles = rles\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.paths[index], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = decode(self.rles[index], image.shape[-2:])\n",
    "        augs = self.transforms(image = image, mask = mask)\n",
    "        return augs['image'], augs['mask'][None].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd84e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:39:47.498421Z",
     "iopub.status.busy": "2024-02-06T13:39:47.498141Z",
     "iopub.status.idle": "2024-02-06T13:39:47.510244Z",
     "shell.execute_reply": "2024-02-06T13:39:47.509421Z"
    },
    "papermill": {
     "duration": 0.020503,
     "end_time": "2024-02-06T13:39:47.512368",
     "exception": false,
     "start_time": "2024-02-06T13:39:47.491865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = {\n",
    "    'train': data.loc[data.stage == 'train'].reset_index(drop=True),\n",
    "    'test': data.loc[data.stage == 'test'].reset_index(drop=True)\n",
    "}\n",
    "\n",
    "\n",
    "ds_train = HiPDataset(\n",
    "    samples['train'].path, samples['train'].vessels, \n",
    "    transforms = config.transforms['train']\n",
    ")\n",
    "ds_test = HiPDataset(\n",
    "    samples['test'].path, samples['test'].vessels, \n",
    "    transforms = config.transforms['test'])\n",
    "\n",
    "num_workers = torch.get_num_threads() * 2\n",
    "dl_train = DataLoader(\n",
    "    ds_train, \n",
    "    shuffle=True, \n",
    "    batch_size = config.batch_size, \n",
    "    drop_last=True, \n",
    "    pin_memory=True, \n",
    "    num_workers = num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05077bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:39:47.526017Z",
     "iopub.status.busy": "2024-02-06T13:39:47.525264Z",
     "iopub.status.idle": "2024-02-06T13:39:47.538014Z",
     "shell.execute_reply": "2024-02-06T13:39:47.537237Z"
    },
    "papermill": {
     "duration": 0.021427,
     "end_time": "2024-02-06T13:39:47.540048",
     "exception": false,
     "start_time": "2024-02-06T13:39:47.518621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "from angionet.utils import cleanup\n",
    "\n",
    "def train(model, loader, criterion, optimizer, scheduler, scoring, accumulate, clipnorm):\n",
    "    model.train()\n",
    "    loss, score = 0.0, 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    pbar = tqdm(loader, desc = 'Training')\n",
    "    for step, (images, masks) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        with torch.autocast(device_type = str(device)):\n",
    "            output = model.forward(images)\n",
    "            running_loss = criterion(output, masks)\n",
    "            running_loss = running_loss / accumulate\n",
    "        \n",
    "        scaler.scale(running_loss).backward()\n",
    "        if (step + 1) % accumulate == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            clip_grad_norm_(model.parameters(), clipnorm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        loss += running_loss.item() * accumulate\n",
    "        running_score = scoring(output.sigmoid(), masks)\n",
    "        score += running_score.item()\n",
    "        pbar.set_postfix(loss = running_loss.item() * accumulate, score = running_score.item())\n",
    "        \n",
    "    loss /= len(loader)\n",
    "    score /= len(loader)\n",
    "    cleanup()\n",
    "    return loss, score\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, dataset, batch_size):\n",
    "    model.eval()\n",
    "    volume = []\n",
    "    nthreads = torch.get_num_threads() * 2\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=nthreads)\n",
    "    for images, masks in tqdm(loader, desc=\"Processing\"):\n",
    "        with torch.autocast(device_type=str(device)):\n",
    "            outputs = model.forward(images.to(device))\n",
    "        outputs = outputs.sigmoid().cpu()\n",
    "        volume.extend(outputs.squeeze(1).numpy())\n",
    "    cleanup()\n",
    "    return np.stack(volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f1e66b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:39:47.552568Z",
     "iopub.status.busy": "2024-02-06T13:39:47.552005Z",
     "iopub.status.idle": "2024-02-06T13:39:54.963084Z",
     "shell.execute_reply": "2024-02-06T13:39:54.962091Z"
    },
    "papermill": {
     "duration": 7.420055,
     "end_time": "2024-02-06T13:39:54.965596",
     "exception": false,
     "start_time": "2024-02-06T13:39:47.545541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddced90db2f24782b983ba036a587bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import timm\n",
    "from segmentation_models_pytorch.encoders._base import EncoderMixin\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class ConvNextEncoder(nn.Module, EncoderMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self._in_channels = 32\n",
    "        self._out_channels = [32, 96, 192, 384, 768]\n",
    "        self._depth = 5\n",
    "        self.backbone = timm.create_model(\n",
    "            \"convnext_tiny\",\n",
    "            pretrained = True, \n",
    "            features_only = True,\n",
    "            in_chans = 32,\n",
    "            patch_size = 4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        stage1, stage2, stage3, stage4 = self.backbone(x)\n",
    "        return [x, stage1, stage2, stage3, stage4]\n",
    "\n",
    "class ConvNextModel(nn.Module):\n",
    "    def __init__(self, decoder_channels = (384, 256, 128, 32), classes = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        smp.encoders.encoders[\"convnext\"] = {\n",
    "            \"encoder\": ConvNextEncoder,\n",
    "            'params' : {},\n",
    "            'pretrained_settings': {},\n",
    "        }\n",
    "        \n",
    "        self.stem = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.model = smp.Unet(\n",
    "            encoder_name='convnext', \n",
    "            encoder_weights = None, \n",
    "            decoder_channels = decoder_channels, \n",
    "            classes = classes, \n",
    "            encoder_depth = 4\n",
    "        )\n",
    "        \n",
    "        self.model.segmentation_head = nn.Identity()\n",
    "        self.convtranspose = nn.ConvTranspose2d(\n",
    "            decoder_channels[-1], \n",
    "            decoder_channels[-1], \n",
    "            kernel_size = 3, \n",
    "            padding = 1, \n",
    "            stride = 2\n",
    "        )\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(decoder_channels[-1] * 2, classes, kernel_size = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        stem = self.stem(x)\n",
    "        x = self.model(stem)\n",
    "        x = self.convtranspose(x, output_size = stem.shape)\n",
    "        x = torch.cat((x, stem), dim = 1)\n",
    "        return self.final_conv(x)\n",
    "    \n",
    "# # ================================= #\n",
    "input = torch.rand((5, 1, 512, 512))\n",
    "model = ConvNextModel()\n",
    "output = model.forward(input)\n",
    "assert output.shape == (5, 1, 512, 512)\n",
    "# # ================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ac68fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:39:54.978635Z",
     "iopub.status.busy": "2024-02-06T13:39:54.978293Z",
     "iopub.status.idle": "2024-02-06T13:39:56.109178Z",
     "shell.execute_reply": "2024-02-06T13:39:56.108406Z"
    },
    "papermill": {
     "duration": 1.139911,
     "end_time": "2024-02-06T13:39:56.111455",
     "exception": false,
     "start_time": "2024-02-06T13:39:54.971544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "T_max = int(len(ds_train) / (config.batch_size * config.accumulate) * config.epochs)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ConvNextModel()\n",
    "model = nn.DataParallel(model).to(device)\n",
    "\n",
    "criterion = smp.losses.DiceLoss(mode = 'binary')\n",
    "metric = dice\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr = config.learning_rate, \n",
    "    weight_decay = config.weight_decay\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max = T_max, eta_min = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa803f1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:39:56.124356Z",
     "iopub.status.busy": "2024-02-06T13:39:56.124004Z",
     "iopub.status.idle": "2024-02-06T13:40:01.059547Z",
     "shell.execute_reply": "2024-02-06T13:40:01.058749Z"
    },
    "papermill": {
     "duration": 4.944738,
     "end_time": "2024-02-06T13:40:01.061939",
     "exception": false,
     "start_time": "2024-02-06T13:39:56.117201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26/4119771261.py:8: NeptuneWarning:\n",
      "\n",
      "The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/segteam/sennet/e/ANG-106\n"
     ]
    }
   ],
   "source": [
    "from neptune_pytorch import NeptuneLogger\n",
    "import neptune\n",
    "from neptune.utils import stringify_unsupported\n",
    "from neptune.types import File\n",
    "from angionet.utils import prettify_transforms\n",
    "\n",
    "NEPTUNE_TOKEN = secrets.get_secret('neptune-token')\n",
    "run = neptune.init_run(\n",
    "    api_token=NEPTUNE_TOKEN,\n",
    "    project=\"segteam/sennet\",\n",
    "    tags=[config.backbone],\n",
    "    capture_hardware_metrics=True\n",
    ")\n",
    "\n",
    "runtime = {\n",
    "    \"model\": type(model).__name__,\n",
    "    \"criterion\": type(criterion).__name__,\n",
    "    \"region-loss\": type(vars(criterion)['_modules'].get(\"region_loss\")).__name__,\n",
    "    \"class-weights\": vars(criterion).get('class_weights'),\n",
    "    \"scoring\": metric.__name__,\n",
    "    \"optimizer\": type(optimizer).__name__,\n",
    "    \"scheduler\": type(scheduler).__name__,\n",
    "}\n",
    "\n",
    "runtime.update({key: value \n",
    "                for key, value in config.to_dict().items() \n",
    "                if key not in ['transforms']})\n",
    "runtime.update(prettify_transforms(config.transforms))\n",
    "\n",
    "run[\"configuration\"] = stringify_unsupported(runtime)\n",
    "run['data/train'].upload(File.as_html(data.query(\"stage == 'train'\")))\n",
    "run['data/test'].upload(File.as_html(data.query(\"stage == 'test'\")))\n",
    "\n",
    "logger = NeptuneLogger(\n",
    "    run=run,\n",
    "    model=model,\n",
    "    log_gradients=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08d4182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:40:01.074824Z",
     "iopub.status.busy": "2024-02-06T13:40:01.074520Z",
     "iopub.status.idle": "2024-02-06T13:40:03.504899Z",
     "shell.execute_reply": "2024-02-06T13:40:03.504073Z"
    },
    "papermill": {
     "duration": 2.439422,
     "end_time": "2024-02-06T13:40:03.507475",
     "exception": false,
     "start_time": "2024-02-06T13:40:01.068053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "H, W = samples['test'][['height', 'width']].iloc[0].astype('int')\n",
    "masks = np.stack([decode(rle, (H, W)) for rle in samples['test'].vessels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f447d55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:40:03.520774Z",
     "iopub.status.busy": "2024-02-06T13:40:03.520445Z",
     "iopub.status.idle": "2024-02-06T13:40:03.527925Z",
     "shell.execute_reply": "2024-02-06T13:40:03.527095Z"
    },
    "papermill": {
     "duration": 0.016563,
     "end_time": "2024-02-06T13:40:03.529855",
     "exception": false,
     "start_time": "2024-02-06T13:40:03.513292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 3):\n",
    "        self.patience = patience\n",
    "        self.epoch = 0\n",
    "        self.iter = 0\n",
    "        self.best = -np.inf\n",
    "        self.msg = \"Objective improved {:.5f} -> {:.5f} at epoch {}\"\n",
    "        self.sigterm = False\n",
    "        \n",
    "    def __call__(self, current):\n",
    "        improvements = False\n",
    "        if current > self.best:\n",
    "            print(self.msg.format(self.best, current, self.epoch))\n",
    "            self.iter = 0\n",
    "            self.best = current\n",
    "            improvements = True\n",
    "        else:\n",
    "            self.iter = self.iter + 1\n",
    "\n",
    "        self.epoch = self.epoch + 1\n",
    "        if self.iter == self.patience:\n",
    "            self.sigterm = True\n",
    "        \n",
    "        return improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d51acf4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T13:40:03.542263Z",
     "iopub.status.busy": "2024-02-06T13:40:03.541741Z",
     "iopub.status.idle": "2024-02-06T16:16:47.840632Z",
     "shell.execute_reply": "2024-02-06T16:16:47.839511Z"
    },
    "papermill": {
     "duration": 9405.037642,
     "end_time": "2024-02-06T16:16:48.572858",
     "exception": false,
     "start_time": "2024-02-06T13:40:03.535216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 447/447 [06:46<00:00,  1.10it/s, loss=0.2, score=0.597]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective improved -inf -> 0.62102 at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 447/447 [06:53<00:00,  1.08it/s, loss=0.094, score=0.84]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective improved 0.62102 -> 0.66933 at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 447/447 [06:49<00:00,  1.09it/s, loss=0.119, score=0.855]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective improved 0.66933 -> 0.71351 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 447/447 [06:49<00:00,  1.09it/s, loss=0.0982, score=0.905]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective improved 0.71351 -> 0.72459 at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 447/447 [06:48<00:00,  1.09it/s, loss=0.079, score=0.806]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective improved 0.72459 -> 0.77110 at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 447/447 [06:52<00:00,  1.08it/s, loss=0.0962, score=0.869]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n",
      "Training: 100%|██████████| 447/447 [06:35<00:00,  1.13it/s, loss=0.0807, score=0.907]\n",
      "Processing: 100%|██████████| 78/78 [02:05<00:00,  1.60s/it]\n",
      "Training: 100%|██████████| 447/447 [06:35<00:00,  1.13it/s, loss=0.119, score=0.848]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective improved 0.77110 -> 0.79454 at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 447/447 [06:48<00:00,  1.09it/s, loss=0.0954, score=0.78]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective improved 0.79454 -> 0.79752 at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 447/447 [06:47<00:00,  1.10it/s, loss=0.0401, score=0.833]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n",
      "Training: 100%|██████████| 447/447 [06:35<00:00,  1.13it/s, loss=0.0822, score=0.761]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective improved 0.79752 -> 0.80746 at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 447/447 [06:46<00:00,  1.10it/s, loss=0.0511, score=0.922]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n",
      "Training: 100%|██████████| 447/447 [06:35<00:00,  1.13it/s, loss=0.0741, score=0.867]\n",
      "Processing: 100%|██████████| 78/78 [02:04<00:00,  1.60s/it]\n",
      "Training: 100%|██████████| 447/447 [06:35<00:00,  1.13it/s, loss=0.0611, score=0.933]\n",
      "Processing: 100%|██████████| 78/78 [02:05<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 9 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 9 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/segteam/sennet/e/ANG-106/metadata\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(patience = 3)\n",
    "for epoch in range(config.epochs):\n",
    "    if es.sigterm:\n",
    "        break\n",
    "    train_loss, train_score = train(\n",
    "        model = model,\n",
    "        loader = dl_train,\n",
    "        optimizer = optimizer,\n",
    "        criterion = criterion,\n",
    "        scoring = metric,\n",
    "        scheduler = scheduler,\n",
    "        accumulate = config.accumulate,\n",
    "        clipnorm = config.clipnorm\n",
    "    )\n",
    "    \n",
    "    output = predict(\n",
    "        model = model, \n",
    "        dataset = ds_test, \n",
    "        batch_size = 16,\n",
    "    )\n",
    "    output = (output > config.thresholds[1]).astype('uint8')\n",
    "    scores = summary(torch.from_numpy(output)[:, :H, :W].contiguous(), torch.from_numpy(masks))\n",
    "\n",
    "    run['train'].append({'loss': train_loss, 'score': train_score})\n",
    "    run['test'].append(scores)\n",
    "    if es(scores['surface-dice']):\n",
    "        filepath = f\"checkpoint-{epoch}.pt\"\n",
    "        torch.save(model, filepath)\n",
    "        run[f'models/checkpoint-{epoch}'].upload(filepath)\n",
    "        indices = np.random.choice(len(ds_test), size = 16, replace = False)\n",
    "        for index in indices:\n",
    "            image, mask = ds_test[index]\n",
    "            masked = colorize(image[0, :H, :W].numpy(), mask[0, :H, :W].byte().numpy(), output[index, :H, :W])\n",
    "            run['test/predictions'].append(File.as_image(masked / 255.0))\n",
    "\n",
    "run['test/highest-score'] = es.best\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6b40d",
   "metadata": {
    "papermill": {
     "duration": 1.222036,
     "end_time": "2024-02-06T16:16:50.901498",
     "exception": false,
     "start_time": "2024-02-06T16:16:49.679462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 6962461,
     "sourceId": 61446,
     "sourceType": "competition"
    },
    {
     "sourceId": 161844638,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 161844679,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 161844693,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9512.444919,
   "end_time": "2024-02-06T16:16:55.979168",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-06T13:38:23.534249",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03a4a276a974496d842baaa25883e347": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_131440ed644b442893e285deb97a8bff",
       "placeholder": "​",
       "style": "IPY_MODEL_48be087cf04e424dbd6a65bef9ebe38b",
       "value": "model.safetensors: 100%"
      }
     },
     "131440ed644b442893e285deb97a8bff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "438c22de4d5c4613ac3f8a7d1cde10ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "483b427f3ac647a999ec38e1fe02d4fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_438c22de4d5c4613ac3f8a7d1cde10ba",
       "placeholder": "​",
       "style": "IPY_MODEL_5bc8267f577e4c6a9c1ea964686fcaed",
       "value": " 114M/114M [00:00&lt;00:00, 249MB/s]"
      }
     },
     "48be087cf04e424dbd6a65bef9ebe38b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4c7649cafa5444408a4e08860422a9a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a1e25d45574466fb7f4abdc99b8a1c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_87a2f38f632740938f5fbb9726bb2e15",
       "max": 114374272.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fbe6ddc858aa42e58964a5170b806adb",
       "value": 114374272.0
      }
     },
     "5bc8267f577e4c6a9c1ea964686fcaed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "87a2f38f632740938f5fbb9726bb2e15": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ddced90db2f24782b983ba036a587bed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_03a4a276a974496d842baaa25883e347",
        "IPY_MODEL_5a1e25d45574466fb7f4abdc99b8a1c2",
        "IPY_MODEL_483b427f3ac647a999ec38e1fe02d4fb"
       ],
       "layout": "IPY_MODEL_4c7649cafa5444408a4e08860422a9a7"
      }
     },
     "fbe6ddc858aa42e58964a5170b806adb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
